"""Runtime inference helpers for local models and token.place relays."""

from __future__ import annotations

import json
import os
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
from typing import Any, Final, Literal

from .notify.tokenplace import TokenPlaceClient, TokenPlaceError

DEFAULT_LOCAL_MAX_TOKENS: Final[int] = 256
DEFAULT_LOCAL_TEMPERATURE: Final[float] = 0.2
DEFAULT_LOCAL_TOP_P: Final[float] = 0.95
DEFAULT_LOCAL_CONTEXT: Final[int] = 4096


class InferenceError(RuntimeError):
    """Raised when inference cannot be executed in the requested mode."""


@dataclass(frozen=True, slots=True)
class InferenceResult:
    """Normalized result for inference helpers."""

    text: str
    mode: Literal["local", "relay"]
    metadata: dict[str, Any]


def _resolve_model_path(model_path: str | os.PathLike[str] | None) -> Path:
    """Return the resolved model path for local inference."""

    if model_path is None:
        env_value = os.getenv("GABRIEL_MODEL_PATH")
        if not env_value:
            raise InferenceError(
                "Set GABRIEL_MODEL_PATH or pass model_path when running local inference."
            )
        model_path = env_value

    resolved = Path(model_path).expanduser().resolve()
    if not resolved.exists():
        raise InferenceError(f"Local model path {resolved} does not exist")
    if not resolved.is_file():
        raise InferenceError(f"Local model path {resolved} is not a file")
    return resolved


@lru_cache(maxsize=1)
def _load_llama_model(
    model_path: str,
    *,
    n_ctx: int,
    n_threads: int | None,
) -> Any:
    """Load and cache a llama.cpp model for local inference."""

    try:
        from llama_cpp import Llama  # type: ignore[import-not-found]
    except ImportError as exc:  # pragma: no cover - exercised via dedicated test
        raise InferenceError(
            "llama-cpp-python is required for local inference; run `pip install llama-cpp-python`."
        ) from exc

    init_kwargs: dict[str, Any] = {
        "model_path": model_path,
        "n_ctx": n_ctx,
    }
    if n_threads is not None:
        init_kwargs["n_threads"] = n_threads
    return Llama(**init_kwargs)


def reset_local_model_cache() -> None:
    """Clear the cached llama.cpp model (intended for tests)."""

    _load_llama_model.cache_clear()


def generate_local_completion(
    prompt: str,
    *,
    model_path: str | os.PathLike[str] | None = None,
    max_tokens: int = DEFAULT_LOCAL_MAX_TOKENS,
    temperature: float = DEFAULT_LOCAL_TEMPERATURE,
    top_p: float = DEFAULT_LOCAL_TOP_P,
    n_ctx: int = DEFAULT_LOCAL_CONTEXT,
    n_threads: int | None = None,
) -> InferenceResult:
    """Return a completion generated by a local llama.cpp model."""

    resolved = _resolve_model_path(model_path)
    llama = _load_llama_model(str(resolved), n_ctx=n_ctx, n_threads=n_threads)

    response = llama(
        prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
    )
    choices = response.get("choices") if isinstance(response, dict) else None
    if not choices:
        raise InferenceError("Local model did not return any choices")

    text_parts: list[str] = []
    for choice in choices:
        if isinstance(choice, dict):
            fragment = choice.get("text")
            if isinstance(fragment, str):
                text_parts.append(fragment)

    if not text_parts:
        raise InferenceError("Local model returned empty text choices")

    text = "".join(text_parts).strip()
    metadata = {
        "model_path": str(resolved),
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "n_ctx": n_ctx,
    }
    if n_threads is not None:
        metadata["n_threads"] = n_threads

    return InferenceResult(text=text, mode="local", metadata=metadata)


def generate_relay_completion(
    prompt: str,
    *,
    base_url: str,
    api_key: str | None = None,
    model: str | None = None,
    temperature: float | None = None,
    metadata: dict[str, Any] | None = None,
    timeout: float = 10.0,
) -> InferenceResult:
    """Relay ``prompt`` to token.place and return the resulting completion."""

    client = TokenPlaceClient(base_url, api_key=api_key, timeout=timeout)
    try:
        completion = client.infer(
            prompt,
            model=model,
            temperature=temperature,
            metadata=metadata,
        )
    except TokenPlaceError as exc:
        raise InferenceError(str(exc)) from exc

    normalized_metadata = {
        "model": completion.model,
        "usage": completion.usage,
        "base_url": base_url,
    }
    if metadata:
        normalized_metadata["metadata"] = metadata
    if model:
        normalized_metadata["requested_model"] = model

    return InferenceResult(text=completion.text, mode="relay", metadata=normalized_metadata)


def run_inference(
    prompt: str,
    *,
    mode: Literal["local", "relay"],
    **kwargs: Any,
) -> InferenceResult:
    """Dispatch inference to the requested backend."""

    if mode == "local":
        return generate_local_completion(prompt, **kwargs)
    if mode == "relay":
        return generate_relay_completion(prompt, **kwargs)
    raise InferenceError(f"Unsupported inference mode: {mode}")


def parse_metadata(metadata_raw: str | None) -> dict[str, Any] | None:
    """Parse a JSON metadata string into a dictionary."""

    if metadata_raw is None:
        return None
    value = metadata_raw.strip()
    if not value:
        return None
    try:
        parsed = json.loads(value)
    except json.JSONDecodeError as exc:  # pragma: no cover - defensive guard
        raise InferenceError("Metadata must be valid JSON") from exc
    if not isinstance(parsed, dict):
        raise InferenceError("Metadata must decode to a JSON object")
    return parsed


__all__ = [
    "DEFAULT_LOCAL_CONTEXT",
    "DEFAULT_LOCAL_MAX_TOKENS",
    "DEFAULT_LOCAL_TEMPERATURE",
    "DEFAULT_LOCAL_TOP_P",
    "InferenceError",
    "InferenceResult",
    "generate_local_completion",
    "generate_relay_completion",
    "parse_metadata",
    "reset_local_model_cache",
    "run_inference",
]
